---
title: Ceph初始
date: 2023-02-20 16:07:59
category: 
  - 集成配置
  - 文件系统
tag: 
  - OSS
  - Ceph
---

<!-- more -->

[[toc]]

## 简介

- [ceph 官网-英文](https://docs.ceph.com/en/latest/)
  > Ceph 是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。
  >
  > 特点：高性能、高可用性、高可扩展性、特性丰富

## Ceph 三种存储类型

| 存储类型           | 特征                                                                                               | 应用场景              | 典型设备      |
| ------------------ | -------------------------------------------------------------------------------------------------- | --------------------- | ------------- |
| 块存储（RBD）      | 存储速度较快<br/>不支持共享存储 [**ReadWriteOnce**]                                                | 虚拟机硬盘            | 硬盘<br/>Raid |
| 文件存储（CephFS） | 存储速度慢（需经操作系统处理再转为块存储）<br/>支持共享存储 [**ReadWriteMany**]                    | 文件共享              | FTP<br/>NFS   |
| 对象存储（Object） | 具备块存储的读写性能和文件存储的共享特性<br/>操作系统不能直接访问，只能通过应用程序级别的 API 访问 | 图片存储<br/>视频存储 | OSS           |

## docker 运行 ceph

```bash
# 参考文档：https://www.codenong.com/cs106379205/

#启动mon
docker run -d --net=host  --name=mon \
-v /opt/ceph/etc:/etc/ceph \
-v /opt/ceph/lib/:/var/lib/ceph/ \
-e MON_IP=172.16.10.20 \
-e CEPH_PUBLIC_NETWORK=172.16.10.0/24 \
ceph/daemon:latest-mimic mon

#启动mgr
docker run -d --net=host --name=mgr \
-v /opt/ceph/etc:/etc/ceph  \
-v /opt/ceph/lib/:/var/lib/ceph  \
ceph/daemon:latest-mimic  mgr

#启动osd 1-3
docker run -d --net=host --name=osd1 \
--privileged=true \
-v /opt/ceph/etc:/etc/ceph  \
-v /opt/ceph/lib/:/var/lib/ceph  \
-v /dev/:/dev/ \
-e OSD_DEVICE=/dev/sdb  \
-e OSD_TYPE=disk \
ceph/daemon:latest-mimic osd
docker run -d --net=host --name=osd2 \
--privileged=true \
-v /opt/ceph/etc:/etc/ceph  \
-v /opt/ceph/lib/:/var/lib/ceph  \
-v /dev/:/dev/ \
-e OSD_DEVICE=/dev/sdc  \
-e OSD_TYPE=disk \
ceph/daemon:latest-mimic osd
docker run -d --net=host --name=osd3 \
--privileged=true \
-v /opt/ceph/etc:/etc/ceph  \
-v /opt/ceph/lib/:/var/lib/ceph  \
-v /dev/:/dev/ \
-e OSD_DEVICE=/dev/sdd  \
-e OSD_TYPE=disk \
ceph/daemon:latest-mimic osd

#启动mds
docker run -d --net=host --name=mds \
-v /opt/ceph/etc:/etc/ceph \
-v /opt/ceph/lib/:/var/lib/ceph/ \
-e CEPHFS_CREATE=1 \
ceph/daemon:latest-mimic mds

#配置ceph
docker exec -it mon bash
#执行以下命令
ceph -s
ceph osd pool set cephfs_data pg_num 64
ceph osd pool set cephfs_data pgp_num 64
ceph osd pool set cephfs_metadata pg_num 32
ceph osd pool set cephfs_metadata pgp_num 32
ceph osd pool set cephfs_metadata min_size 1
ceph osd pool set cephfs_data min_size 1

#映射到客户端磁盘目录
#获取口令
cat /opt/ceph/etc/ceph.client.admin.keyring
#基于上面的口令加载磁盘
mount -t ceph 192.168.60.100:6789:/ /data -o name=admin,secret=AQDLp19jQYDUJBAAGgesnbf1D9A2g1FVW0DPSw==
```

## 常用命令

### 集群命令

```bash
## 检查ceph的状态(常用)
ceph -s
ceph status
ceph health
ceph health detail
## 实时观察集群健康状态(常用)
ceph -w
## 查看ceph存储空间
ceph df

## 查看ceph集群中的认证用户及相关的key(常用)
ceph auth list
## 查看某一用户详细信息
ceph auth get client.admin
## 只查看用户的key信息
ceph auth print-key client.admin

## 查看ceph log日志所在的目录
ceph-conf --name mon.node1 --show-config-value log_file
```

### mon 命令

```bash
## 查看mon的状态信息
ceph mon stat
## 查看mon的选举状态
ceph quorum_status
## 查看mon的映射信息
ceph mon dump
## 删除一个mon节点
ceph mon remove node1
```

### msd 命令

```bash
## 查看msd状态
ceph mds stat
## 查看msd的映射信息
ceph mds dump
```

### osd 命令

```bash
## 查看ceph osd运行状态
ceph osd stat
## 查看osd映射信息
ceph osd dump
ceph osd crush dump
## 查看osd的目录树
ceph osd tree
ceph osd ls-tree rack1  # 查看osd tree中rack1下的osd编号
## 查看osd各硬盘使用率
ceph osd df
## 查看osd延时
ceph osd perf
```

### PG 组命令

```bash
1、查看pg组的映射信息
# ceph pg dump
2、查看一个PG的map
# ceph pg map 2.c
```

### pool 命令

```bash
## 查看ceph集群中的pool数量
ceph osd lspools

## 在ceph集群中创建一个pool   这里的100指的是PG组
ceph osd pool create jiayuan 100
## 为一个ceph pool配置配额
ceph osd pool set-quota data max_objects 10000
## 在集群中删除一个pool
## 备注：删除pool需要在配置文件ceph.conf中 [mon]添加mon allow pool delete = true并重启mon服务, 如systemctl restart ceph-mon.target
ceph osd pool delete testpool testpool  --yes-i-really-really-mean-it  #集群名字需要重复两次

## 显示集群中pool的详细信息
rados df
## 查看data池的pg数量
ceph osd pool get volumes pg_num
## 设置data池的最大存储空间为100T（默认是1T)
ceph osd pool set data target_max_bytes 100000000000000
## 设置data池的副本数是1
ceph osd pool set jiayuan size 1
## 设置data池能接受写操作的最小副本为1
ceph osd pool set jiayuan min_size 1
## 查看集群中所有pool的副本尺寸
ceph osd dump | grep 'replicated size'
## 设置一个pool的pg数量
ceph osd pool set jiayuan pg_num 128
## 设置一个pool的pgp数量
ceph osd pool set jiayuan pgp_num 128
## 查询public与cluster是否分开，在存储节点执行以下命令
ps aux | grep osd
```

### rbd 命令

```bash
## 查看ceph中一个pool里的所有镜像
rbd ls volumes
## 查看ceph pool中一个镜像的信息
rbd info -p volumes --image volume-2203c1c8-64ea-4da2-934c-011ba9f99603
## 在test池中创建一个命名为test2的10000M的镜像
## 备注：有些操作系统需要强制指定特性, 加上–image-feature layering ：
rbd create -p volumes --size 10000 test2
## rbd create -p volumes --size 10000 test2 --image-feature layering

## 删除一个镜像
rbd rm -p volumes test2

## 调整一个镜像的尺寸
rbd resize -p volumes --size 20000 test2

## 镜像快照的创建、查询、删除
# 创建：
rbd snap create volumes/test2@snap1
# 查询：
rbd snap -p volumes ls test2
# 删除：
rbd snap rm volumes/test2@snap1
# 删除一个镜像文件的所有快照：
rbd snap purge -p volumes test2

## 导出/导入镜像
# 导出：
rbd export -p volumes test2 /root/test2.raw
# 导入：
rbd import /root/test2.raw -p volumes --image test-import
```
