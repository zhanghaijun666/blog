---
title: Ceph安装-deploy
date: 2023-02-20 16:07:59
star: true
category: 
  - 集成配置
  - 文件系统
tag: 
  - OSS
  - Ceph
---


<!-- more -->
[[toc]]

## 节点规划

| 主机名 | public-ip     | cluster-ip    | 磁盘                                | 角色                                                |
| ------ | ------------- | ------------- | ----------------------------------- | --------------------------------------------------- |
| ceph1  | 192.168.60.11 | 192.168.20.11 | 系统盘: sda   <br />osd盘: sdb、sdc | monitor,mgr,rgw,mds,osd<br />NTP Server,ceph-deploy |
| ceph2  | 192.168.60.12 | 192.168.20.12 | 系统盘: sda   <br />osd盘: sdb、sdc | monitor,mgr,rgw,mds,osd                             |
| ceph3  | 192.168.60.13 | 192.168.20.13 | 系统盘: sda   <br />osd盘: sdb、sdc | monitor,mgr,rgw,mds,osd                             |

### 节点角色

- ceph-deploy：Ceph集群部署节点，负责集群整体部署，也可以复用cpeh集群中的节点作为部署节点。
- NTP Server：时间同步源
- monitor：Ceph监视管理节点，承担Ceph集群重要的管理任务，一般需要3或5个节点。
- mgr： Ceph 集群管理节点（manager），为外界提供统一的入口。
- rgw: Ceph对象网关，是一种服务，使客户端能够利用标准对象存储API来访问Ceph集群
- mds：Ceph元数据服务器，MetaData Server，主要保存的文件系统服务的元数据，使用文件存储时才需要该组件
- osd：Ceph存储节点Object Storage Daemon，实际负责数据存储的节点。

## 系统配置

### 网络设置

```bash
## 在每个节点上配置ip地址，方式可和下面的方式不同
nmcli d
nmcli dev status
nmcli connection show
## 显示所有网络连接信息
nmcli -p connection show
## 查看所有的网卡设备
nmcli -p device status 
## 查看指定网卡详细信息
nmcli device show ens37 #显示指定网卡的详细信息
nmcli device show #显示所有网卡的详细信息
nmcli connection down ens37 #停用网络连接
nmcli connection up ens37 #启用网络连接
## 设置IP地址
nmcli con modify enp0s3 ipv4.method manual ipv4.addess "192.168.60.11/24" ipv4.gateway "192.168.200.1" connection.autoconnect yes
## 启用新配置，使新地址生效
nmcli con up enp0s3

# 配置IP地址
ssh root@192.168.60.11 " \
nmcli con mod ens18 ipv4.addresses 192.168.60.11/24; \
nmcli con mod ens18 ipv4.gateway 192.168.60.1; \
nmcli con mod ens18 ipv4.method manual; \
nmcli con mod ens18 ipv4.dns '8.8.8.8'; \
nmcli con up ens18"
ssh root@192.168.60.12 " \
nmcli con mod ens18 ipv4.addresses 192.168.60.11/24; \
nmcli con mod ens18 ipv4.gateway 192.168.60.1; \
nmcli con mod ens18 ipv4.method manual; \
nmcli con mod ens18 ipv4.dns '8.8.8.8'; \
nmcli con up ens18"
ssh root@192.168.60.13 " \
nmcli con mod ens18 ipv4.addresses 192.168.60.11/24; \
nmcli con mod ens18 ipv4.gateway 192.168.60.1; \
nmcli con mod ens18 ipv4.method manual; \
nmcli con mod ens18 ipv4.dns '8.8.8.8'; \
nmcli con up ens18"
```

### host配置

```bash
## 修改主机名 分别在三节点上执行
## 主机名需要和host设置相同，必须设置否则无法初始化，后续也有问题！
hostnamectl set-hostname ceph1
hostnamectl set-hostname ceph2
hostnamectl set-hostname ceph3

cat >>/etc/hosts <<EOF
## Ceph Public Network
192.168.60.11  ceph1
192.168.60.12  ceph2
192.168.60.13  ceph3
## Ceph Cluster Network
192.168.20.11  ceph1
192.168.20.12  ceph2
192.168.20.13  ceph3
EOF
```

### 节点互信

```bash
## 生产密钥
ssh-keygen -t rsa
## 密钥交换
ssh-copy-id ceph1
ssh-copy-id ceph2
ssh-copy-id ceph3
## 密钥交换，密码是123456 上面和下面二择一即可
for i in ceph1 ceph2 ceph3 ;do
expect -c "spawn ssh-copy-id -i root@$i
        expect {
                "*yes/no*" {send "yesr"; exp_continue}
                "*password*" {send "123456r"; exp_continue}
                "*Password*" {send "123456r";}
        } "
done
## 验证互信
ssh ceph1 date && ssh ceph2 date && ssh cep3 date
```

### 系统参数设置

```bash
## 关闭selinux
# setenforce 0 && sed -i 's/^SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config
## 修改系统参数
echo "* soft nproc 65535" >> /etc/security/limits.conf
echo "* hard nproc 65535" >> /etc/security/limits.conf
echo "* soft nofile 65535" >> /etc/security/limits.conf
echo "* hard nofile 65535" >> /etc/security/limits.conf
```

### 防火墙设置

```bash
## 防火墙策略
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.200.10" accept'
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.200.11" accept'
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.200.12" accept'
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.20.10" accept'
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.20.11" accept'
firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="192.168.20.12" accept'
firewall-cmd --reload
## 查看当前防火墙策略：
firewall-cmd --list-all

systemctl stop firewalld
systemctl disable --now firewalld
iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat
iptables -P FORWARD ACCEPT
```

### 时间同步

```bash
## 时间同步
yum -y install ntp ntpdate
## 节点1
##注释其他时间源
sed -i 's/^server/#server/g' /etc/ntp.conf
echo "restrict 192.168.100.10 mask 255.255.255.0 nomodify notrap" >> /etc/ntp.conf
## 127.127.1.0 回环地址 作为时钟源
# echo "server 127.127.1.0 prefer" >> /etc/ntp.conf
echo "server 127.127.1.0 iburst" >> /etc/ntp.conf
## 填入127.127.1.0 回环地址，fudge设置时间服务器的层级 stratum 0~15  ,0：表示顶级 , 10：通常用于给局域网主机提供时间服务
echo "Fudge 127.127.1.0 stratum 10 " >> /etc/ntp.conf
## 启动ntpd服务
systemctl start ntpd
## 节点2和节点3上执行
sed -i 's/^server/#server/g' /etc/ntp.conf
echo "server 192.168.100.10 iburst" >> /etc/ntp.conf
echo "Fudge 192.168.100.10 stratum 10" >> /etc/ntp.conf
## 启动ntpd服务
systemctl start ntpd
```

### yum源设置

```bash
#配置centos、epeo、ceph源
curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
wget -O /etc/yum.repos.d/ceph.repo http://down.i4t.com/ceph/ceph.repo
yum clean all
yum makecache

## 本地yum源
## https://github.com/emikulic/darkhttpd
yun install-y darkhttpd
## 启动http并指定本地的rpm源位置（/root/ceph_rhel77/ceph-rpms）
darkhttpd /root/ceph_rhel77/ceph-rpms --port 8081 --daemon

cd /etc/yum.repos.d/
[root@ceph1 yum.repos.d]
echo "
[ceph]
name=ceph
baseurl=http://192.168.200.10:8081
enabled=1
gpgcheck=0
" > /etc/yum.repos.d/ceph.repo
yum makecache && yum update
```

## 创建用户

```bash
## 创建yunwei组
groupadd -g 1090 yunwei
## 创建yunwei用户   
useradd -g yunwei -u 1090 yunwei
## 设置yunwei密码 
echo "123456Aa@" | passwd --stdin yunwei
```

## 安装Ceph

### 配置ceph-deploy

```bash
## 安装 ceph-deploy
yum install -y python-setuptools ceph-deploy
## 校验版本
ceph-deploy --version
## 创建集群目录
mkdir -p /root/ceph-deploy && cd /root/ceph-deploy
# 初始化集群 参数设置 --cluster-network 集群对外的网络,用于业务访问 --public-network 集群内通信的网络
ceph-deploy new ceph1 ceph2 ceph3 --public-network 192.168.60.0/24 --cluster-network 192.168.20.0/24
ls
# ceph.conf     ceph配置文件
# ceph-deploy-ceph.log   ceph日志文件
# ceph.mon.keyring   keyring主要做身份验证
## 添加允许ceph时间偏移
# echo "mon clock drift allowed = 2" >>/root/ceph-deploy/ceph.conf
# echo "mon clock drift warn backoff = 30" >>/root/ceph-deploy/ceph.conf

## 在所有节点执行
# yum install -y ceph ceph-fuse rbd ceph-radosgw
yum install -y ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds
## 当然如果你不在乎网络问题，也可以使用官方推荐的安装方式,下面的方式会重新给我们配置yum源，这里不太推荐
## –no-adjust-repos 是直接使用本地源，不生成官方源
# ceph-deploy install --no-adjust-repos ceph1 ceph2 ceph3
```

### 监控节点（monitor）

> 官方介绍：为了获得高可用性，您应该运行带有至少三个监视器的生产Ceph集群。
>
> 维护着展示集群状态的各种图表， 包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。Ceph 保存着发生在Monitors、OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。监视器还负责管理守护进程和客户端之间的身份验证。

```bash
## 在节点1上执行。
cd /root/ceph-deploy
ceph-deploy mon create-initial
```

### 管理服务（mgr）

> 负责跟踪运行时指标和Ceph 集群的当前状态，包括存储利用率，当前性能指标和系统负载。Ceph Manager守护进程还托管基于python的插件来管理和公开Ceph集群信息，包括基于Web的Ceph Manager Dashboard和 REST API。

```bash
## 在节点1上执行。
## 配置manager节点
ceph-deploy mgr create ceph1 ceph2 ceph3
## 拷贝 配置文件及密钥到其他 monitor节点
ceph-deploy admin ceph1 ceph2 ceph3
## 查看集群及服务状态
ceph -s
# health: 显示集群当前状态。
# services:显示的mon、mgr、mds、osd服务状态。
# io:     显示当前的IO读写速率
```

### 创建OSD

> 主要是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向Ceph Monitors 提供一些监控信息。
>
> 冗余和高可用性通常至少需要3个Ceph OSD。

```bash
ssh ceph1 lsblk
ssh ceph2 lsblk
ssh ceph3 lsblk

## 在三个节点上执行lsblk查看磁盘信息，将sdb和sdc磁盘作为集群的osd存储使用。
## 在节点1上执行。
## ceph1创建OSD
ceph-deploy osd create ceph1 --data /dev/sdb
ceph-deploy osd create ceph1 --data /dev/sdc
## ceph2创建OSD
ceph-deploy osd create ceph2 --data /dev/sdb
ceph-deploy osd create ceph2 --data /dev/sdc
## ceph3创建OSD
ceph-deploy osd create ceph3 --data /dev/sdb
ceph-deploy osd create ceph3 --data /dev/sdc
## 创建完成后，查看OSD信息：
ssh ceph1 ceph -s
ssh ceph2 ceph -s
ssh ceph3 ceph -s
## 查看osd列表：
ssh ceph1 ceph osd tree
ssh ceph2 ceph osd tree
ssh ceph3 ceph osd tree
```

### 创建MDS

> 用于cephfs文件系统

```bash
## 在节点1上执行
ceph-deploy mds create ceph1 ceph2 ceph3
```

### 服务验证

```bash
## 在三个节点上执行。
## 检查mon服务
systemctl status ceph-mon.target
## 检查mds服务
systemctl status ceph-mds.target
## 检查mgr服务
systemctl status ceph-mgr.target
```

## 使用场景

### CephFS文件系统

> 主要应用于网络共享存储。

```bash
###### 创建Pool
## PG总数= (OSD数*100) /最大副本数/池数(结果必须舍入到最接近2的N次幂的值) 
ceph osd pool create cephfs_data 32
pool 'cephfs_data' created
ceph osd pool create cephfs_metadata 32
pool 'cephfs_metadata' created
###### 创建ceph文件系统
ceph fs new cephfs cephfs_metadata cephfs_data
## ceph文件系统查看及状态
ceph fs status
###### ceph文件系统挂载
## 将ceph文件系统挂载在三个节点的目录下，测试文件系统是否正常运行
## 创建挂载目录
ssh ceph1 mkdir -p /mycephfs
ssh ceph1 mkdir -p /mycephfs
ssh ceph1 mkdir -p /mycephfs
## 挂载cephfs至/mycephfs目录
## 1) 查看管理员密钥
cat /etc/ceph/ceph.client.admin.keyring 
# [client.admin]
# key = AQDYYNtj75TmNRAAaScLPYSsbtTIKE4at2Zdyg==
# 将密钥保存为文件（三个节点执行）
ssh ceph1 echo " AQDYYNtj75TmNRAAaScLPYSsbtTIKE4at2Zdyg==" > /etc/ceph/ceph.secret
ssh ceph2 echo " AQDYYNtj75TmNRAAaScLPYSsbtTIKE4at2Zdyg==" > /etc/ceph/ceph.secret
ssh ceph3 echo " AQDYYNtj75TmNRAAaScLPYSsbtTIKE4at2Zdyg==" > /etc/ceph/ceph.secret
## 2）挂载ceph文件系统
ssh ceph1 mount -t ceph cloud1:6789:/ /mycephfs -o name=admin,secretfile=/etc/ceph/ceph.secret
ssh ceph2 mount -t ceph cloud1:6789:/ /mycephfs -o name=admin,secretfile=/etc/ceph/ceph.secret
ssh ceph3 mount -t ceph cloud1:6789:/ /mycephfs -o name=admin,secretfile=/etc/ceph/ceph.secret
## 3）查看挂载目录
ssh ceph1 df -h
ssh ceph2 df -h
ssh ceph3 df -h
```

### 对象存储

> 主要应用于图片，视频，镜像、日志等对象存储。

```bash
###### 部署rgw集群
## 在节点1上执行
cd /root/ceph-deploy
ceph-deploy rgw create cloud1 cloud2 cloud3
## 修改默认端口
## 部署完成后默认端口是7480，将其修改为80
## 修改/root/ceph-deploy目录下的ceph.conf文件，执行如下添加内容：
cd /root/ceph-deploy
cat >> ceph.conf << EOF
[client.rgw.cloud1]
rgw_frontends = "civetweb port=80"
[client.rgw.cloud2]
rgw_frontends = "civetweb port=80"
[client.rgw.cloud3]
rgw_frontends = "civetweb port=80"
EOF
## 将配置文件推送集群其他节点
## 在节点1上执行
ceph-deploy --overwrite-conf config push cloud1 cloud2 cloud3
## 重启三个节点的radosgw服务
ssh ceph1 systemctl restart ceph-radosgw.target
ssh ceph2 systemctl restart ceph-radosgw.target
ssh ceph3 systemctl restart ceph-radosgw.target
## 测试使用root账户进行集群访问验证
ceph -s -k /var/lib/ceph/radosgw/ceph-rgw.cloud1/keyring --name client.rgw.cloud1

## 使用s3 api访问对象存储
## 在节点1上执行。
## 创建radosgw用户
radosgw-admin user create --uid=radosgw --display-name='radosgw' -k /var/lib/ceph/radosgw/ceph-rgw.cloud1/keyring --name client.rgw.cloud1
## 创建完成之后需要把access_key和secret_key保存下来，也可以使用下面的命令来查看
radosgw-admin user info --uid=radosgw --display-name='radosgw' -k /var/lib/ceph/radosgw/ceph-rgw.cloud1/keyring --name client.rgw.cloud1

## 测试S3接口是否可用，采用s3cmd进行测试
## 安装s3cmd软件
## 节点1执行
rpm -ivh /root/s3cmd-2.3.0-1.el7.noarch.rpm
## 生成配置文件
s3cmd --configure
## 需要修改的内容：
Access Key: D028HA7T16KJHU2602YA                      # 粘贴服务端生成的Access Key
Secret Key: RWczKVORMdDBw2mtgLs2dUPq2xrCehnjOtB6pHPY  # 粘贴服务端生成的Secret Key
Default Region [US]:                                  # 直接回车即可
S3 Endpoint [s3.amazonaws.com]: 192.168.200.10        # 输入对象存储的IP地址
DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]:  %(bucket).192.168.200.10    
 # 输入对象存储的bucket地址
Encryption password:                              # 空密码回车
Path to GPG program [/usr/bin/gpg]:               # /usr/bin/gpg命令路径 回车
Use HTTPS protocol [Yes]: no                      # 是否使用https，选no
HTTP Proxy server name:                           # haproxy 留空回车
Test access with supplied credentials? [Y/n] n
Save settings? [y/N] y                           # y 要保存配置文件
Configuration saved to '/root/.s3cfg'            # 最后配置文件保存的位置/root.s3cfg

## s3验证基础操作
s3cmd mb s3://my-bucket                 # 创建my-bucket桶
s3cmd ls                                # 查看所有的桶
s3cmd put /etc/hosts s3://my-bucke      # 向指定桶中上传/etc/hosts文件
s3cmd ls s3://my-bucket                 # 显示my-bucket中的文件
s3cmd del s3://my-bucket/hosts          # 删除my-bucket中的hosts文件
s3cmd rb s3://my-bucket                 # 删除my-bucket
```

### 块存储

> 主要应用于k8s，openstack云计算等场景

```bash
## 创建pool
ceph osd pool create rbd 32 32
ceph osd pool application enable rbd rbd
## 创建块设备镜像
rbd create --size 5000M img01   #创建一个5000MB的块设备。
rbd info img01                  #查看镜像信息
## 镜像映射
rbd feature disable img01 exclusive-lock, object-map, fast-diff, deep-flatten
rbd map img01
# /dev/rbd0
## 格式化块设备镜像
mkfs.xfs /dev/rbd0
## 挂载至本地
mount /dev/rbd0 /media/
df -h
```
