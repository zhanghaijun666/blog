---
order: 23
title: Docker搭建Ceph集群(nautilus版本)
date: 2024-06-12 13:21:48
tags: 
  - 文件系统
---
## 环境

创建三台虚拟机，本教程采用的Linux是[CentOS7](https://so.csdn.net/so/search?q=CentOS7&spm=1001.2101.3001.7020).6版本，Docker的版本是19.03.13，Ceph的版本是nautilus。三台虚拟机的情况如下：

| 主机名称 | 主机IP          | 说明                                           |
| -------- | --------------- | ---------------------------------------------- |
| ceph1    | 192.168.161.137 | 容器主节点(Dashbaord、mon、mds、rgw、mgr、osd) |
| ceph2    | 192.168.161.135 | 容器子节点(mon、mds、rgw、mgr、osd)            |
| ceph3    | 192.168.161.136 | 容器子节点(mon、mds、rgw、mgr、osd)            |

## 预检

部署Ceph之前我们需要对自身机器的环境做一个预检查。主要涉及到防火墙，主机名等设置。

1. 关闭防火墙

```bash
systemctl stop firewalld && systemctl disable firewalld
```

1. 关闭selinux(linux的安全子系统)

```bash
sed -i 's/enforcing/disabled/' /etc/selinux/config && setenforce 0
```

PS: 正式环境实际部署时，最好通过加入IP白名单的方式来操作，而不是直接关闭防火墙。
\3. 设置主机名，分别把三台虚拟机的主机名设置成ceph1,ceph2,ceph3。

```bash
hostnamectl set-hostname ceph1
hostnamectl set-hostname ceph2
hostnamectl set-hostname ceph3
```

1. 在主节点ceph1配置免密登录到ceph2和ceph3,下面命令在主节点ceph1上执行。

```bash
#在192.168.161.133（ceph1）上执行：
ssh-keygen
#把密钥发给ceph2、ceph3
ssh-copy-id ceph2 
ssh-copy-id ceph3
```

1. 在三个节点上分别执行下列命令配置host。

```bash
cat >> /etc/hosts <<EOF
192.168.161.137 ceph1
192.168.161.135 ceph2
192.168.161.136 ceph3
EOF
```

1. 内核参数优化

```bash
#调整内核参数
cat >> /etc/sysctl.conf << EOF
kernel.pid_max=4194303
vm.swappiness = 0
EOF
sysctl -p
# read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，根据一些Ceph的公开分享，8192是比较理想的值
echo "8192" > /sys/block/sda/queue/read_ahead_kb
# I/O Scheduler，关于I/O Scheculder的调整，简单说SSD要用noop，SATA/SAS使用deadline。
echo "deadline" > /sys/block/sda/queue/scheduler
echo "noop" > /sys/block/sda/queue/scheduler
1234567891011
```

1. 打开ntp服务
   ntp服务的作用是用于同步不同机器的时间。

```bash
#查看ntp，如果状态是inactive，则表示没启动
systemctl status ntpd
#启动ntp服务
systemctl start ntpd
#设置开启自启动ntp服务
systemctl enable ntpd

ntpq -pn
```

1. 其他配置
   把容器内的 ceph 命令 alias 到本地，方便使用，其他 ceph 相关命令也可以参考添加：

```bash
echo 'alias ceph="docker exec mon ceph"' >> /etc/profile
source /etc/profile
```

上面的预检做完之后，下面我们就开始具体的部署。

## 部署

### 1. 创建Ceph目录

在宿主机上创建Ceph目录与容器建立映射，便于直接操纵管理Ceph配置文件，以root身份依次在三台节点上创建/usr/local/ceph/{admin,data, etc,lib, logs}目录：

```bash
mkdir -p /usr/local/ceph/{admin,data,etc,lib,logs}
```

该命令会一次创建5个指定的目录，注意逗号分隔，不能有空格。
其中：

- admin文件夹下用于存储启动脚本，
- data文件夹用于挂载文件，
- etc文件夹下存放了ceph.conf等配置文件
- lib文件夹下存放了各组件的密钥文件
- logs文件夹下存放了ceph的日志文件。

> 对`docker`内用户进行授权

```bash
chown -R 167:167 /usr/local/ceph/  #docker内用户id是167，这里进行授权
```

### 2. 安装docker

1. 首先通过使用官方安装脚本安装

```bash
curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
```

2. 启动Docker

```bash
sudo systemctl start docker
```

3. 设置开机自启动

```bash
systemctl enable docker
```

4. 使用docker加速器， 在/etc/docker/daemon.json 中写入如下内容（如果文件不存在则请创建该文件）

```
mkdir /etc/docker
vim /etc/docker/daemon.json
```

创建好文件之后再该文件中配置加速器，这里用的是网易云和百度云的加速器

```json
{"registry-mirrors": ["https://hub-mirror.c.163.com","https://mirror.baidubce.com"]}
```

需要注意的是一定要保证该文件符合json规范，否则Docker将不能启动。之后重启服务

5. 重启服务

```bash
# 使配置生效
systemctl daemon-reload
# 重启docker服务
systemctl restart docker
# 将docker服务设置成开机自启动
systemctl enable docker
```

### 3. 创建OSD磁盘

1. 创建OSD磁盘

> OSD服务是对象存储守护进程，负责把对象存储到本地文件系统，必须要有一块独立的磁盘作为存储。

2. 如果没有独立磁盘，我们可以在Linux下面创建一个虚拟磁盘进行挂载，步骤如下：

   2.1. 初始化10G的镜像文件：

   ```bash
   mkdir -p /usr/local/ceph-disk
   dd if=/dev/zero of=/usr/local/ceph-disk/ceph-disk-01 bs=1G count=10
   ```

2.2. 将镜像文件虚拟成块设备：

   ```bash
losetup -f /usr/local/ceph-disk/ceph-disk-01
   ```

2.3. 格式化（名称根据fdisk -l进行查询）：

   ```bash
mkfs.xfs -f /dev/loop0
   ```

2.4. 挂载文件系统，就是将loop0磁盘挂载到/usr/local/ceph/data/osd/目录下：

   ```bash
mount /dev/loop0 /usr/local/ceph/data/osd/
   ```

3. 如果有独立磁盘的话，那就简单一点
   3.1 直接格式化（ 名称根据fdisk -l进行查询）

   ```
   mkfs.xfs -f /dev/sdb
   ```

   3.2 挂载文件系统：

   ```
   mount /dev/sdb /usr/local/ceph/data/osd/
   ```

4. 可以通过`df -h`命令查看挂载结果

### 4. 拉取ceph

这里用到了 dockerhub 上最流行的 ceph/daemon 镜像（这里需要拉取nautilus版本的ceph，latest-nautilus）

```bash
docker pull ceph/daemon:latest-nautilus
```

### 5. 编写脚本（脚本都放在admin文件夹下）

1. start_mon.sh

```bash
#!/bin/bash
docker run -d --net=host \
    --name=mon \
    -v /etc/localtime:/etc/localtime \
    -v /usr/local/ceph/etc:/etc/ceph \
    -v /usr/local/ceph/lib:/var/lib/ceph \
    -v /usr/local/ceph/logs:/var/log/ceph \
    -e MON_IP=192.168.161.137,192.168.161.135,192.168.161.136 \
    -e CEPH_PUBLIC_NETWORK=192.168.161.0/24 \
    ceph/daemon:latest-nautilus  mon
```

> 这个脚本是为了启动监视器，监视器的作用是维护整个Ceph集群的全局状态。一个集群至少要有一个监视器，最好要有奇数个监视器。方便当一个监视器挂了之后可以选举出其他可用的监视器。

启动脚本说明：

- name参数，指定节点名称，这里设为mon
- -v xxx:xxx 是建立宿主机与容器的目录映射关系，包含 etc、lib、logs目录。
- `MON_IP`是Docker运行的IP地址（通过ifconfig来查询，取ens33里的inet那个IP）,这里我们有3台服务器，那么MAN_IP需要写上3个IP，如果IP是跨网段的`CEPH_PUBLIC_NETWORK`必须写上所有网段。
- `CEPH_PUBLIC_NETWORK`配置了运行Docker主机所有网段
  这里必须指定nautilus版本，不然会默认操作最新版本ceph，mon必须与前面定义的name保持一致。
  
2. start_osd.sh

```bash
#!/bin/bash
docker run -d \
    --name=osd \
    --net=host \
    --restart=always \
    --privileged=true \
    --pid=host \
    -v /etc/localtime:/etc/localtime \
    -v /usr/local/ceph/etc:/etc/ceph \
    -v /usr/local/ceph/lib:/var/lib/ceph \
    -v /usr/local/ceph/logs:/var/log/ceph \
    -v /usr/local/ceph/data/osd:/var/lib/ceph/osd \
    ceph/daemon:latest-nautilus  osd_directory  
```

这个脚本是用于启动OSD组件的，OSD（Object Storage Device）是RADOS组件，其作用是用于存储资源。
脚本说明：
\1. name 是用于指定OSD容器的名称
\2. net 是用于指定host，就是前面我们配置host
\3. restart指定为always，使osd组件可以在down时重启。
4.privileged是用于指定该osd是专用的。
这里我们采用的是`osd_directory` 镜像模式，如果有独立的磁盘我们可以用`osd_ceph_disk`模式，不需要格式化，直接指定设备名称即可 `OSD_DEVICE=/dev/sdb`
**3. start_mgr.sh**

```bash
#!/bin/bash
docker run -d --net=host  \
  --name=mgr \
  -v /etc/localtime:/etc/localtime \
  -v /usr/local/ceph/etc:/etc/ceph \
  -v /usr/local/ceph/lib:/var/lib/ceph \
  -v /usr/local/ceph/logs:/var/log/ceph \
  ceph/daemon:latest-nautilus mgr
```

> 这个脚本是用于启动mgr组件，它的主要作用是分担和扩展monitor的部分功能，提供图形化的管理界面以便我们更好的管理ceph存储系统。其启动脚本比较简单，在此不再赘述。

**4. start_rgw.sh**

```bash
#!/bin/bash
docker run \
    -d --net=host \
    --name=rgw \
    -v /etc/localtime:/etc/localtime \
    -v /usr/local/ceph/etc:/etc/ceph \
    -v /usr/local/ceph/lib:/var/lib/ceph \
    -v /usr/local/ceph/logs:/var/log/ceph \
    ceph/daemon:latest-nautilus rgw
```

> 该脚本主要是用于启动rgw组件，rgw（Rados GateWay）作为对象存储网关系统，一方面扮演RADOS集群客户端角色，为对象存储应用提供数据存储，另一方面扮演HTTP服务端角色，接受并解析互联网传送的数据。

**5. start_mds.sh**

```bash
#!/bin/bash
docker run -d \
   --net=host \
   --name=mds \
   --privileged=true \
   -v /etc/localtime:/etc/localtime \
   -v /usr/local/ceph/etc:/etc/ceph \
   -v /usr/local/ceph/lib:/var/lib/ceph \
   -v /usr/local/ceph/logs:/var/log/ceph \
   -e CEPHFS_CREATE=0 \
   -e CEPHFS_METADATA_POOL_PG=512 \
   -e CEPHFS_DATA_POOL_PG=512 \
   ceph/daemon:latest-nautilus  mds
```

该脚本是用来启动mds组件，该组件的作用是
1、跟踪文件层次结构并存储只供CephFS使用的元数据，
2、mds不直接给客户端提供数据，英雌可以避免系统中的单点故障。
脚本说明：
`CEPHFS_CREATE` 是为METADATA服务生成文件系统， 0表示不自动创建文件系统（默认值）， 1表示自动创建。
`CEPHFS_DATA_POOL_PG`是数据池的数量，默认为8。
`CEPHFS_METADATA_POOL_PG`是元数据池的数量，默认为8。

### 6. 执行脚本

#### 启动mon

1. 首先在主节点ceph1上执行start_mon.sh脚本，启动后通过`docker ps -a|grep mon`查看启动结果，启动成功之后生成配置数据，在ceph主配置文件中，追加如下内容：

```bash
cat >>/usr/local/ceph/etc/ceph.conf <<EOF
# 容忍更多的时钟误差
mon clock drift allowed = 2
mon clock drift warn backoff = 30
# 允许删除pool
mon_allow_pool_delete = true

[mgr]
# 开启WEB仪表盘
mgr modules = dashboard
[client.rgw.ceph1]
# 设置rgw网关的web访问端口
rgw_frontends = "civetweb port=20003"
EOF
```

1. 拷贝所有数据（已包含脚本）到另外2台服务器

```bash
scp -r /usr/local/ceph ceph2:/usr/local/
scp -r /usr/local/ceph ceph3:/usr/local/
```

1. 通过远程ssh，在ceph2和ceph3上依次启动mon(启动前不要修改ceph.conf文件)

```bash
ssh ceph2 bash /usr/local/ceph/admin/start_mon.sh
ssh ceph3 bash /usr/local/ceph/admin/start_mon.sh
```

启动后通过 `ceph -s`查看集群状态，如果能够看到ceph2和ceph3,则表示集群创建成功，此时的状态应该是HEALTH_OK状态。

#### 启动OSD

在执行start_osd.sh脚本之前，首先需要在mon节点生成osd的密钥信息，不然直接启动会报错。命令如下：

```bash
docker exec -it mon ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring
```

接着在主节点下执行如下命令：

```bash
bash /usr/local/ceph/admin/start_osd.sh
ssh ceph2 bash /usr/local/ceph/admin/start_osd.sh
ssh ceph3 bash /usr/local/ceph/admin/start_osd.sh
```

全部osd都启动之后，稍等片刻后，执行`ceph -s`查看状态，应该可以看到多了如下信息（总共3个osd）

```text
  osd: 3 osds: 3 up, 3 in 
```

PS: osd的个数最好维持在奇数个。

#### 启动mgr

直接在主节点ceph1上执行如下三个命令：

```bash
bash /usr/local/ceph/admin/start_mgr.sh
ssh ceph2 bash /usr/local/ceph/admin/start_mgr.sh
ssh ceph3 bash /usr/local/ceph/admin/start_mgr.sh
```

#### 启动rgw

同样的我们首先还是需要先在mon节点生成rgw的密钥信息，命令如下：

```bash
docker exec mon ceph auth get client.bootstrap-rgw -o /var/lib/ceph/bootstrap-rgw/ceph.keyring
```

接着在主节点ceph1上执行如下三个命令：

```bash
bash /usr/local/ceph/admin/start_rgw.sh
ssh ceph2 bash /usr/local/ceph/admin/start_rgw.sh
ssh ceph3 bash /usr/local/ceph/admin/start_rgw.sh
```

#### 启动mds

直接在主节点ceph1上执行如下三个命令

```bash
bash /usr/local/ceph/admin/start_mds.sh
ssh ceph2 bash /usr/local/ceph/admin/start_mds.sh
ssh ceph3 bash /usr/local/ceph/admin/start_mds.sh
```

启动完成之后再通过`ceph-s`查看集群的状态
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019140108649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQ1MzQ4MDg=,size_16,color_FFFFFF,t_70#pic_center)

## 安装Dashboard管理后台

首先确定主节点，通过`ceph -s`命令查看集群状态，找到mgr为active的那个节点，如下：

```bash
  mgr: ceph1(active), standbys: ceph2, ceph3
```

这里的主节点就是ceph1节点。

1. 开启dashboard功能

```bash
docker exec mgr ceph mgr module enable dashboard
```

1. 创建登录用户与密码

```bash
docker exec mgr ceph dashboard set-login-credentials admin test
```

这里设置用户名为admin,密码为test。
\3. 配置外部访问端口个，这里指定端口号是18080，可以自定义修改

```bash
docker exec mgr ceph config set mgr mgr/dashboard/server_port 18080
```

1. 配置外部访问地址，这里我的主节点IP是192.168.161.137，你需要换成自己的IP地址。

```bash
docker exec mgr ceph config set mgr mgr/dashboard/server_addr 192.168.161.137
```

1. 关闭https(如果没有证书或内网访问， 可以关闭)

```bash
docker exec mgr ceph config set mgr mgr/dashboard/ssl false
```

1. 重启Mgr DashBoard服务

```bash
docker restart mgr
```

1. 查看Mgr DashBoard服务

```bash
docker exec mgr ceph mgr services
```

最后通过 <http://192.168.161.137:18080/#/dashboard> 访问。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019135105282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQ1MzQ4MDg=,size_16,color_FFFFFF,t_70#pic_center)

## 创建FS文件系统

在主节点执行即可

1. 创建Data Pool:

```bash
docker exec osd ceph osd pool create cephfs_data 128 128
```

1. 创建Metadata Pool:

```bash
docker exec osd ceph osd pool create cephfs_metadata 128 128
```

注意： 如果受mon_max_pg_per_osd限制， 不能设为128，可以调小点， 改为64。
\3. 创建CephFS

```bash
docker exec osd ceph fs new cephfs cephfs_metadata cephfs_data
```

将上面的数据池与元数据池关联， 创建cephfs的文件系统。
\4. 查看FS信息

```bash
[root@ceph1 ceph]# docker exec osd ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
```

## 查看整个集群信息

至此，整个集群就已经搭建完毕，通过`ceph -s`命令，可以查看整个集群信息，我们规划的所有节点都已创建成功并加入集群
