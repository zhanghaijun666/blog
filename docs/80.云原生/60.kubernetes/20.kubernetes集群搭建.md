---
title: kubernetes集群搭建
date: 2023-02-20 16:07:59
permalink: /pages/80/60/20/
category: 
  - 云原生
  - kubernetes
tag: 
  - kubernetes
---

<!-- more -->

[[toc]]

## 环境准备

- 节点 CPU 核数必须是 ：>= 2 核 ，否则 k8s 无法启动
- DNS 网络： 最好设置为 本地网络连通的 DNS,否则网络不通，无法下载一些镜像
- linux 内核： linux 内核必须是 4 版本以上，因此必须把 linux 核心进行升级

> 准备 3 台虚拟机环境，或者是 3 台阿里云服务器都可。

- k8s-master01: 此机器用来安装 k8s-master 的操作环境
- k8s-node01: 此机器用来安装 k8s node 节点的环境
- k8s-node02: 此机器用来安装 k8s node 节点的环境

## 依赖环境

```bash
# 安装依赖环境，注意：每一台机器都需要安装此依赖环境
yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git iproute lrzsz bash-completion tree bridge-utils unzip bind-utils gcc

# 关闭防火墙firewalld
systemctl stop firewalld && systemctl disable firewalld
# 安装iptables && 启动iptables && 开机自启 && 清空iptables规则 && 保存配置
yum -y install iptables-services && systemctl start iptables && systemctl enable iptables && iptables -F && service iptables save

# 关闭swap分区【虚拟内存】并且永久关闭虚拟内存
swapoff -a && sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
# 关闭selinux
setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config

# 升级Linux内核版本
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm
# 安装内核
yum --enablerepo=elrepo-kernel install -y kernel-lt
# 查询已安装的内核
rpm -qa | grep kernel
# 查看默认启动项
awk -F\' '$1=="menuentry " {print $2}' /etc/grub2.cfg
# 设置开机从新内核启动
# grub2-set-default 'CentOS Linux (4.4.189-1.el7.elrepo.x86_64) 7 (Core)'
# 上述命令不生效，可执行下面的命令设置默认启动
# 默认启动的顺序是从0开始（CentOS Linux (3.10.0-1127.el7.x86_64) 7），新内核是从头插入，所以需要选择0
grub2-set-default 0
# 注意：设置完内核后，需要重启服务器才会生效。
reboot
# 查询内核 4.4.249-1.el7.elrepo.x86_64
uname -r

# 调整内核参数，对于k8s
cat > /root/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF

# 将优化内核文件拷贝到/etc/sysctl.d/文件夹下，这样优化文件开机的时候能够被调用
cp /root/kubernetes.conf /etc/sysctl.d/kubernetes.conf
# 手动刷新，让优化文件立即生效
sysctl -p /etc/sysctl.d/kubernetes.conf

#########################################################################

# 设置系统时区为中国/上海（可略过）
timedatectl set-timezone Asia/Shanghai
# 将当前的 UTC 时间写入硬件时钟（可略过）
timedatectl set-local-rtc 0
# 重启依赖于系统时间的服务（可略过）
systemctl restart rsyslog
systemctl restart crond

# 关闭系统不需要的服务
systemctl stop postfix && systemctl disable postfix

#########################################################################

# 设置日志保存方式
# 1）.创建保存日志的目录
mkdir /var/log/journal
# 2）.创建配置文件存放目录
mkdir /etc/systemd/journald.conf.d
# 3）.创建配置文件
cat > /etc/systemd/journald.conf.d/99-prophet.conf <<EOF
[Journal]
Storage=persistent
Compress=yes
SyncIntervalSec=5m
RateLimitInterval=30s
RateLimitBurst=1000
SystemMaxUse=10G
SystemMaxFileSize=200M
MaxRetentionSec=2week
ForwardToSyslog=no
EOF
# 4）.重启systemd journald的配置
systemctl restart systemd-journald

#########################################################################

# 打开文件数调整 (忽略，不执行，使用默认即可)
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf

# kube-proxy 开启 ipvs 前置条件
modprobe br_netfilter

cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

# 使用lsmod命令查看这些文件是否被引导
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

```

## docker 依赖安装

```bash
# 安装docker
yum install -y yum-utils device-mapper-persistent-data lvm2

# 紧接着配置一个稳定（stable）的仓库、仓库配置会保存到/etc/yum.repos.d/docker-ce.repo文件中
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

#更新Yum安装的相关Docke软件包&安装Docker CE
yum update -y && yum install docker-ce

# 设置docker daemon文件
# 1) 创建/etc/docker目录
mkdir /etc/docker
# 2) 更新daemon.json文件
cat > /etc/docker/daemon.json <<EOF
{"exec-opts": ["native.cgroupdriver=systemd"],"log-driver": "json-file","log-opts": {"max-size": "100m"}}
EOF
# 注意： 一定注意编码问题，出现错误：查看命令：journalctl -amu  docker 即可发现错误

# 创建，存储docker配置文件
mkdir -p /etc/systemd/system/docker.service.d

# 重启docker服务
systemctl daemon-reload && systemctl restart docker && systemctl enable docker
```

## kubeadm

```bash
# 安装kubernetes的时候，需要安装kubelet, kubeadm等包，但k8s官网给的yum源是packages.cloud.google.com，国内访问不了，此时我们可以使用阿里云的yum仓库镜像。
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 安装kubeadm、kubelet、kubectl
yum install -y kubeadm-1.15.1 kubelet-1.15.1 kubectl-1.15.1
# 启动 kubelet
systemctl enable kubelet && systemctl start kubelet
```

## 集群安装

```bash
# 查看所依赖的docker镜像（需要联网）
kubeadm config images list
# 如下示例
k8s.gcr.io/kube-apiserver:v1.15.1
k8s.gcr.io/kube-controller-manager:v1.15.1
k8s.gcr.io/kube-scheduler:v1.15.1
k8s.gcr.io/kube-proxy:v1.15.1
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.3.10
k8s.gcr.io/coredns:1.3.1
```

```bash
# 下载镜像
docker pull k8s.gcr.io/kube-apiserver:v1.15.1
docker pull k8s.gcr.io/kube-controller-manager:v1.15.1
docker pull k8s.gcr.io/kube-scheduler:v1.15.1
docker pull k8s.gcr.io/kube-proxy:v1.15.1
docker pull k8s.gcr.io/pause:3.1
docker pull k8s.gcr.io/etcd:3.3.10
docker pull k8s.gcr.io/coredns:1.3.1
```

> kubeadm 初始化 k8s 集群的时候，会从 gce Google 云中下载（pull）相应的镜像,且镜像相对比较大，下载比较慢，且需要解决科学上网的一个问题，国内上 goole，懂得...

```bash
# 编写批量导入本地的镜像脚本（sh脚本文件：image-load.sh）

#!/bin/bash
#注意 镜像解压的目录位置
ls /root/kubeadm-basic.images > /tmp/images-list.txt
cd /root/kubeadm-basic.images
for i in $(cat /tmp/images-list.txt)
do
  docker load -i $i
done
rm -rf /tmp/images-list.txt

#########################################################################

# 修改权限，可执行权限
chmod 755 image-load.sh
# 开始执行,镜像导入
./image-load.sh
# 导入成功后查看镜像文件
docker images
```

## 复制三个虚拟机

> 复制 1 个 master（192.16.18.111）
>
> 复制 2 个 node（192.16.18.112/192.16.18.113）
>
> 注意启动的时候，看好内核选择，要选择 4.4.249-1.el7.elrepo.x86_64

```bash
# 分别设置hostname
hostnamectl set-hostname k8s-master-111
hostnamectl set-hostname k8s-node-112
hostnamectl set-hostname k8s-node-113

# 查看机器的hostname
hostname
# 设置hosts文件，追加三台机器（三台机器都要设置）
vi /etc/hosts
192.16.18.111 k8s-master-111
192.16.18.112 k8s-node-112
192.16.18.113 k8s-node-113

```

## k8s 部署（只需要在主节点执行）

```bash
# 拉去yaml资源配置文件
kubeadm config print init-defaults > kubeadm-config.yaml

# 修改yaml资源文件
localAPIEndpoint:
  advertiseAddress: 192.168.66.10     # 注意：修改配置文件的IP地址
kubernetesVersion: v1.15.1            #注意：修改版本号，必须和kubectl版本保持一致
networking:
  podSubnet: "10.244.0.0/16"          # 指定flannel模型通信 pod网段地址,此网段和flannel网段一致
  serviceSubnet: "10.96.0.0/12"
# 追加如下，指定使用ipvs网络进行通信
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: kubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs

# 初始化主节点，开始部署
kubeadm init --config=kubeadm-config.yaml --experimental-upload-certs | tee kubeadm-init.log
# 注意：执行此命令，CPU核心数量必须大于1核，否则无法执行成功
```

> 按照 k8s 指示，执行下面的命令：

```bash
# 初始化成功后执行如下命令
# 创建目录，保存连接配置缓存，认证文件
mkdir -p $HOME/.kube
# 拷贝集群管理配置文件
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# 授权给配置文件
chown $(id -u):$(id -g) $HOME/.kube/config
```

```bash
kubectl get node
# 执行上面的命令会显示 k8s-master-111   NotReady   master   2m17s   v1.15.1
# 我们发现已经可以成功查询node节点信息了，但是节点的状态却是NotReady,不是Runing的状态。原因是此时我们使用ipvs+flannel的方式进行网络通信，但是flannel网络插件还没有部署，因此节点状态此时为NotReady
```

## flannel 插件

> 遇到下载不了的问题，请参考：[部署 k8s 的时候 kube-flannel.yml 下载不下来解决 8](https://blog.csdn.net/chen_haoren/article/details/108580338)

```bash
#部署flannel网络插件 --- 只需要在主节点执行
# 下载flannel网络插件
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# 部署flannel
kubectl create -f kube-flannel.yml
# 查看flannel是否下载完毕，Running标识下载完成
kubectl get pod -n kube-system
# 下载完毕，再次查看node
kubectl get node
# 按照k8s指示，再node节点执行下面的命令：(命令会有差异，根据自身的机器提示来执行)
# 找不到此命令，可再日志文件中查看：`cat kubeadm-init.log`
kubeadm join 192.16.18.111:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:aecd3ce263208637621c846a37ce3651a6ddf6c3daaa89fb679803a733261e7e
# 查看节点，其他的node已经添加进来，发现还有一些节点处于NotReady状态，是因为这些节点pod容器还处于初始化的状态，需要等一点时间：
kubectl get node

# 查询工作空间中pod容器的详细信息
kubectl get pod -n kube-system -o wide

# 也可进行部署网络（略过）
# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```
